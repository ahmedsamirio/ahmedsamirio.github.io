{"title":"Finetuing an Egyptian Arabic Translator","markdown":{"yaml":{"title":"Finetuing an Egyptian Arabic Translator","author":"Ahmed Samir","date":"July 21nd, 2024","format":{"html":{"code-fold":true}}},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\nThe following blogpost contains a walkthrough of a recent project that I worked on to learn more about finetuning as part of the [`Mastering LLMs: A Conference For Developers & Data Scientists`](https://maven.com/parlance-labs/fine-tuning) course by Hamel Husain and Dan Becker.\n\nInstruction tuning datasets have become abundant since the release of open source models, and many individual and group attempts have been made to curate and translate these datasets into Arabic.\n\nUnfortunately, Egyptian Arabic dialect is almost forgotten in these attempts, and this project is an attempt to drive a change in this direction.\n\nAt the moment, you can use a flagship model from either OpenAI or Anthropic to translate from English to Egyptian Arabic and obtain really nice results, but the large cost makes it unfeasible for individuals to translate the available opensource datasets.\n\nThe outcome of this project is creating this translator using open source model to make it easier to translate the massive amounts of data available and integrating the Egyptian Arabic dialect into open source models instruction finetuning.\n\n# Outline\n\n#### The blogpost will be split into 3 major parts:\n\n1. Creating finetuning data using GPT-4o\n2. Preparing the dataset for finetuning\n3. Finetuning Llama 3 8B using Axolotl\n4. Comparing the results before and after finetuning\n\n# Creating finetuing data using GPT-4o\n\nIn order to finetune a translation model, we need to have translation pairs between English and Egyptian Arabic.\n\nSince my intention with this model was to use it in translating instruction and conversation datasets, I had to choose something to begin with. \n\nTo be honest, I arbitrarly chose the open assistant dataset, and even took a random sample from the messages instead of sampling entire conversations. Anyways, in future iterations on this project, I plan to diversify the sources of translation used for finetuning, for example using different datasets such as alpaca, or using wikipedia articles.\n\n\nWhile testing GPT-4o for translating from English to Egyptian Arabic, I noticed that GPT-4o tends to generate bad translations if asked to direct translation, which could be mitigated by first translating into Arabic, and then translating into Egyptian Arabic. Therefore, I used GPT-4o to translate the sentence of paragraph into Arabic, and then to Egyptian Arabic.\n\nYou can check the prompt used below.\n\nAnd here, you can see a sample of GPT-4o's translation using that prompt.\n\nAfter developing the prompt, I utilized the openai batch api to directly translate a sample of 10K messages from the dataset as follows.\n\nThe following snippet simply does 3 things:\n1. It generate 10 separate jsonl files, where each file contains prompts separate requests for translating different messages.\n2. It submits these jsonl files to openai's batch api\n3. It downloads their results\n\n# Preparing the dataset for finetuning\n\nThe output from the previous step consists of separate jsonl files, where each files contains the output for a specific batch. In order to proceed to the finetuning part, we need to process this output into a suitable format that we can deal with.\n\nLet's take a look into the batch inputs and outputs.\n\nSo for each sample in these batch files, we need to extract the English text used for translation, and the JSON output from GPT-4o, and parse out the Arabic and Egyptian Arabic translations.\n\nFor each triplet, I create 6 translation pairs:\n1. Arabic to English\n2. Egyptian Arabic to English\n3. English to Arabic\n4. Egyptian Arabic to Arabic\n5. English to Egyptian Arabic\n6. Arabic to Egyptian Arabic\n\nThe main purpose of the model was to translate from English to Egyptian Arabic, but I though that including the back translation could enhance the model capabilities, and also produce the bridge of translating from English to Arabic, and from Arabic to Egyptian Arabic, which proved to do well with GPT-4o.\n\nDue to some JSON parsing errors, the final datasset was around 57K rows instead of 60K.\n\nThe next step was to split this dataset into train and test, in order to be able to validate the performance of the model after finetuning.\n\nAnd the final part was to convert these train and test sets into HuggingFace datasets, so that we can easily use them in Axolotl.\n\n# Finetuning Llama 3 8B using Axolotl\n\nTo give you a brief intro about Axolotl, it is a tool designed to streamline LLM finetuning. I like Axolotl because it lets you focus on the data instead of focusing on the finetuning code, while having the best finetuning practicies built-in.\n\nIn this project, I used a pretty simple finetuning configuration that I'll provide below. To summarize what the configuration entails:\n1. It loads a Llama 3 8B in 8bit\n2. It uses the [alpaca format](https://github.com/tatsu-lab/stanford_alpaca) for finetuning\n3. It finetunes a [LoRA](https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html) adapter instead of doing a full finetune\n4. It uses [sample packing](https://axolotl-ai-cloud.github.io/axolotl/docs/multipack.html) to improve finetuning efficiency\n5. It trains the model for 2 epochs, while running 10 evals per epoch\n6. It logs the train and eval loss into weights and biases\n\nThe finetuning was carried out on a single A5000 GPU (24 GB VRAM) on [Jarvis Labs](https://jarvislabs.ai/) that costs 0.49$/hr, and took around 10 hours to complete. You can check the weights and biases log over [here](https://wandb.ai/ahmedsamirio/en_eg_translator/runs/hwzxxt0r).\n\nFor more info about Axolotl, I highly recommend the [documentation](https://axolotl-ai-cloud.github.io/axolotl/), and checking this [short video guide](https://www.youtube.com/watch?v=HAYPoeC41fw) by [Jarvis Labs](https://jarvislabs.ai/) that shows how to spin up an instance that uses axolotl over there.\n\n```yaml\nbase_model: meta-llama/Meta-Llama-3-8B\nmodel_type: LlamaForCausalLM\ntokenizer_type: AutoTokenizer\n\nload_in_8bit: true\nload_in_4bit: false\nstrict: false\n\ndatasets:\n  - path: translation-dataset-v3-train.hf\n    type: alpaca\n    train_on_split: train\n\ntest_datasets:\n  - path: translation-dataset-v3-test.hf\n    type: alpaca\n    split: train\n\ndataset_prepared_path: ./last_run_prepared\noutput_dir: ./llama_3_translator\nhub_model_id: ahmedsamirio/llama_3_translator_v3\n\n\nsequence_len: 2048\nsample_packing: true\npad_to_sequence_len: true\neval_sample_packing: false\n\nadapter: lora\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_linear: true\nlora_fan_in_fan_out:\nlora_target_modules:\n  - gate_proj\n  - down_proj\n  - up_proj\n  - q_proj\n  - v_proj\n  - k_proj\n  - o_proj\n\nwandb_project: en_eg_translator\nwandb_entity: ahmedsamirio\nwandb_name: llama_3_en_eg_translator_v3\n\ngradient_accumulation_steps: 4\nmicro_batch_size: 2\nnum_epochs: 2\noptimizer: paged_adamw_32bit\nlr_scheduler: cosine\nlearning_rate: 2e-5\n\ntrain_on_inputs: false\ngroup_by_length: false\nbf16: auto\nfp16:\ntf32: false\n\ngradient_checkpointing: true\nearly_stopping_patience:\nresume_from_checkpoint:\nlocal_rank:\nlogging_steps: 1\nxformers_attention:\nflash_attention: true\n\nwarmup_steps: 10\nevals_per_epoch: 10\neval_table_size:\neval_max_new_tokens: 128\nsaves_per_epoch: 1\ndebug:\ndeepspeed:\nweight_decay: 0.0\nfsdp:\nfsdp_config:\nspecial_tokens:\n  pad_token: <|end_of_text|>\n```\n\n\n# Comparing the finetuned model with GPT-4o\n\nOf course, the comparison here will go in facor of GPT-4o, but since we were aiming to emulate it's performance, let's make a comparison to see how far off our finetuned model is.\n\nI'll use three random sample responses from the [alpaca dataset](https://huggingface.co/datasets/tatsu-lab/alpaca).\n\n### Sample 1\n\n### Sample 2\n\n### Sample 3\n\n# Conclusion\n\nAs you can see, the results are still not that great, however the finetuned model is almost catching up with GPT-4o, and I believe that with some minor tweaks in the finetuning methodology, we can achieve a performance close to that of GPT-4o.\n\n# TL;DR\n\n- Used GPT-4o to create translation pairs from English to Modern Standard Arabic and then to Egyptian Arabic.\n- Generated a dataset from the OpenAssistant/oasst2 messages using GPT-4o and OpenAI's batch API.\n- Prepared and processed the dataset for fine-tuning.\n- Fine-tuned Llama-3-8B using Axolotl, focusing on LoRA adapters and sample packing.\n- Evaluated the fine-tuned model against GPT-4o using sample texts.\n- Found that while the fine-tuned model isn't perfect, it's a significant step towards accessible Egyptian Arabic translations.\n","srcMarkdownNoYaml":"\n\n# Introduction\n\nThe following blogpost contains a walkthrough of a recent project that I worked on to learn more about finetuning as part of the [`Mastering LLMs: A Conference For Developers & Data Scientists`](https://maven.com/parlance-labs/fine-tuning) course by Hamel Husain and Dan Becker.\n\nInstruction tuning datasets have become abundant since the release of open source models, and many individual and group attempts have been made to curate and translate these datasets into Arabic.\n\nUnfortunately, Egyptian Arabic dialect is almost forgotten in these attempts, and this project is an attempt to drive a change in this direction.\n\nAt the moment, you can use a flagship model from either OpenAI or Anthropic to translate from English to Egyptian Arabic and obtain really nice results, but the large cost makes it unfeasible for individuals to translate the available opensource datasets.\n\nThe outcome of this project is creating this translator using open source model to make it easier to translate the massive amounts of data available and integrating the Egyptian Arabic dialect into open source models instruction finetuning.\n\n# Outline\n\n#### The blogpost will be split into 3 major parts:\n\n1. Creating finetuning data using GPT-4o\n2. Preparing the dataset for finetuning\n3. Finetuning Llama 3 8B using Axolotl\n4. Comparing the results before and after finetuning\n\n# Creating finetuing data using GPT-4o\n\nIn order to finetune a translation model, we need to have translation pairs between English and Egyptian Arabic.\n\nSince my intention with this model was to use it in translating instruction and conversation datasets, I had to choose something to begin with. \n\nTo be honest, I arbitrarly chose the open assistant dataset, and even took a random sample from the messages instead of sampling entire conversations. Anyways, in future iterations on this project, I plan to diversify the sources of translation used for finetuning, for example using different datasets such as alpaca, or using wikipedia articles.\n\n\nWhile testing GPT-4o for translating from English to Egyptian Arabic, I noticed that GPT-4o tends to generate bad translations if asked to direct translation, which could be mitigated by first translating into Arabic, and then translating into Egyptian Arabic. Therefore, I used GPT-4o to translate the sentence of paragraph into Arabic, and then to Egyptian Arabic.\n\nYou can check the prompt used below.\n\nAnd here, you can see a sample of GPT-4o's translation using that prompt.\n\nAfter developing the prompt, I utilized the openai batch api to directly translate a sample of 10K messages from the dataset as follows.\n\nThe following snippet simply does 3 things:\n1. It generate 10 separate jsonl files, where each file contains prompts separate requests for translating different messages.\n2. It submits these jsonl files to openai's batch api\n3. It downloads their results\n\n# Preparing the dataset for finetuning\n\nThe output from the previous step consists of separate jsonl files, where each files contains the output for a specific batch. In order to proceed to the finetuning part, we need to process this output into a suitable format that we can deal with.\n\nLet's take a look into the batch inputs and outputs.\n\nSo for each sample in these batch files, we need to extract the English text used for translation, and the JSON output from GPT-4o, and parse out the Arabic and Egyptian Arabic translations.\n\nFor each triplet, I create 6 translation pairs:\n1. Arabic to English\n2. Egyptian Arabic to English\n3. English to Arabic\n4. Egyptian Arabic to Arabic\n5. English to Egyptian Arabic\n6. Arabic to Egyptian Arabic\n\nThe main purpose of the model was to translate from English to Egyptian Arabic, but I though that including the back translation could enhance the model capabilities, and also produce the bridge of translating from English to Arabic, and from Arabic to Egyptian Arabic, which proved to do well with GPT-4o.\n\nDue to some JSON parsing errors, the final datasset was around 57K rows instead of 60K.\n\nThe next step was to split this dataset into train and test, in order to be able to validate the performance of the model after finetuning.\n\nAnd the final part was to convert these train and test sets into HuggingFace datasets, so that we can easily use them in Axolotl.\n\n# Finetuning Llama 3 8B using Axolotl\n\nTo give you a brief intro about Axolotl, it is a tool designed to streamline LLM finetuning. I like Axolotl because it lets you focus on the data instead of focusing on the finetuning code, while having the best finetuning practicies built-in.\n\nIn this project, I used a pretty simple finetuning configuration that I'll provide below. To summarize what the configuration entails:\n1. It loads a Llama 3 8B in 8bit\n2. It uses the [alpaca format](https://github.com/tatsu-lab/stanford_alpaca) for finetuning\n3. It finetunes a [LoRA](https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html) adapter instead of doing a full finetune\n4. It uses [sample packing](https://axolotl-ai-cloud.github.io/axolotl/docs/multipack.html) to improve finetuning efficiency\n5. It trains the model for 2 epochs, while running 10 evals per epoch\n6. It logs the train and eval loss into weights and biases\n\nThe finetuning was carried out on a single A5000 GPU (24 GB VRAM) on [Jarvis Labs](https://jarvislabs.ai/) that costs 0.49$/hr, and took around 10 hours to complete. You can check the weights and biases log over [here](https://wandb.ai/ahmedsamirio/en_eg_translator/runs/hwzxxt0r).\n\nFor more info about Axolotl, I highly recommend the [documentation](https://axolotl-ai-cloud.github.io/axolotl/), and checking this [short video guide](https://www.youtube.com/watch?v=HAYPoeC41fw) by [Jarvis Labs](https://jarvislabs.ai/) that shows how to spin up an instance that uses axolotl over there.\n\n```yaml\nbase_model: meta-llama/Meta-Llama-3-8B\nmodel_type: LlamaForCausalLM\ntokenizer_type: AutoTokenizer\n\nload_in_8bit: true\nload_in_4bit: false\nstrict: false\n\ndatasets:\n  - path: translation-dataset-v3-train.hf\n    type: alpaca\n    train_on_split: train\n\ntest_datasets:\n  - path: translation-dataset-v3-test.hf\n    type: alpaca\n    split: train\n\ndataset_prepared_path: ./last_run_prepared\noutput_dir: ./llama_3_translator\nhub_model_id: ahmedsamirio/llama_3_translator_v3\n\n\nsequence_len: 2048\nsample_packing: true\npad_to_sequence_len: true\neval_sample_packing: false\n\nadapter: lora\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_linear: true\nlora_fan_in_fan_out:\nlora_target_modules:\n  - gate_proj\n  - down_proj\n  - up_proj\n  - q_proj\n  - v_proj\n  - k_proj\n  - o_proj\n\nwandb_project: en_eg_translator\nwandb_entity: ahmedsamirio\nwandb_name: llama_3_en_eg_translator_v3\n\ngradient_accumulation_steps: 4\nmicro_batch_size: 2\nnum_epochs: 2\noptimizer: paged_adamw_32bit\nlr_scheduler: cosine\nlearning_rate: 2e-5\n\ntrain_on_inputs: false\ngroup_by_length: false\nbf16: auto\nfp16:\ntf32: false\n\ngradient_checkpointing: true\nearly_stopping_patience:\nresume_from_checkpoint:\nlocal_rank:\nlogging_steps: 1\nxformers_attention:\nflash_attention: true\n\nwarmup_steps: 10\nevals_per_epoch: 10\neval_table_size:\neval_max_new_tokens: 128\nsaves_per_epoch: 1\ndebug:\ndeepspeed:\nweight_decay: 0.0\nfsdp:\nfsdp_config:\nspecial_tokens:\n  pad_token: <|end_of_text|>\n```\n\n\n# Comparing the finetuned model with GPT-4o\n\nOf course, the comparison here will go in facor of GPT-4o, but since we were aiming to emulate it's performance, let's make a comparison to see how far off our finetuned model is.\n\nI'll use three random sample responses from the [alpaca dataset](https://huggingface.co/datasets/tatsu-lab/alpaca).\n\n### Sample 1\n\n### Sample 2\n\n### Sample 3\n\n# Conclusion\n\nAs you can see, the results are still not that great, however the finetuned model is almost catching up with GPT-4o, and I believe that with some minor tweaks in the finetuning methodology, we can achieve a performance close to that of GPT-4o.\n\n# TL;DR\n\n- Used GPT-4o to create translation pairs from English to Modern Standard Arabic and then to Egyptian Arabic.\n- Generated a dataset from the OpenAssistant/oasst2 messages using GPT-4o and OpenAI's batch API.\n- Prepared and processed the dataset for fine-tuning.\n- Fine-tuned Llama-3-8B using Axolotl, focusing on LoRA adapters and sample packing.\n- Evaluated the fine-tuned model against GPT-4o using sample texts.\n- Found that while the fine-tuned model isn't perfect, it's a significant step towards accessible Egyptian Arabic translations.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"notebook.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.54","theme":"cosmo","title-block-banner":true,"title":"Finetuing an Egyptian Arabic Translator","author":"Ahmed Samir","date":"July 21nd, 2024"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}
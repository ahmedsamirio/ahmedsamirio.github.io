{"title":"Finetuing an Egyptian Arabic Translator","markdown":{"yaml":{"title":"Finetuing an Egyptian Arabic Translator","author":"Ahmed Samir","date":"2024-07-20","format":{"html":{"code-fold":true}}},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\nThis blog post contains a walkthrough of a recent project I worked on to dive deeper into fine-tuning as part of the [`Mastering LLMs: A Conference For Developers & Data Scientists`](https://maven.com/parlance-labs/fine-tuning) course by Hamel Husain and Dan Becker.\n\nWith the abundance of instruction tuning datasets following the release of open source models, many efforts have been made to curate and translate these datasets into Arabic. Unfortunately, Egyptian Arabic dialect is almost forgotten in these attempts, and this project aims to change that.\n\nCurrently, you can use flagship models from OpenAI or Anthropic to translate from English to Egyptian Arabic with excellent results. However, the high cost makes it impractical for individuals to translate the available open source datasets.\n\nThis project's outcome is a translator built using an open source model, making it easier to translate large amounts of data and integrating the Egyptian Arabic dialect into open source models for instruction fine-tuning.\n\n# Outline\n\n#### The blogpost will be split into 3 major parts:\n\n1. Creating finetuning data using GPT-4o\n2. Preparing the dataset for finetuning\n3. Finetuning Llama 3 8B using Axolotl\n4. Comparing the results before and after finetuning\n\n# Creating finetuing data using GPT-4o\n\nTo fine-tune a translation model, we need translation pairs between English and Egyptian Arabic.\n\nSince my goal was to use this model for translating instruction and conversation datasets, I needed a starting point.\n\nI decided to use the OpenAssistant dataset, selecting a random sample of messages rather than entire conversations. In future iterations of this project, I plan to diversify the sources of translation data, using different datasets like Alpaca or even Wikipedia articles.\n\nWhile testing GPT-4o for translating from English to Egyptian Arabic, I noticed that direct translations often resulted in poor quality. To mitigate this, I found that translating first into Modern Standard Arabic and then into Egyptian Arabic produced much better results. Therefore, I used GPT-4o to translate each sentence or paragraph into Arabic first, and then into Egyptian Arabic.\n\nHere's the prompt I used:\n\nAnd here, you can see a sample of GPT-4o's translation using that prompt.\n\nAfter developing the prompt, I utilized the OpenAI batch API to translate a sample of 10K messages from the dataset. The process involved three main steps:\n\n1. Generating 10 separate JSONL files, each containing prompts for translating different messages.\n2. Submitting these JSONL files to OpenAI's batch API.\n3. Downloading the results.\n\nHere’s the code snippet that accomplishes this:\n\n# Preparing the dataset for finetuning\n\nThe output from the previous step consists of separate JSONL files, each containing the output for a specific batch. To proceed with fine-tuning, we need to process this output into a suitable format.\n\nHere's a look at the batch inputs and outputs:\n\nFor each sample in these batch files, we need to extract the English text used for translation and the JSON output from GPT-4o, then parse out the Arabic and Egyptian Arabic translations.\n\nFor each triplet, I create 6 translation pairs:\n\n1. Arabic to English\n2. Egyptian Arabic to English\n3. English to Arabic\n4. Egyptian Arabic to Arabic\n5. English to Egyptian Arabic\n6. Arabic to Egyptian Arabic\n\nThe main purpose of the model was to translate from English to Egyptian Arabic, but I thought that including the back translation could enhance the model's capabilities. Additionally, it creates a bridge for translating from English to Arabic and from Arabic to Egyptian Arabic, which proved effective with GPT-4o.\n\nHere is the updated code to handle this process:\n\nThis snippet loads the batch inputs and outputs, storing them in separate lists. Now we need to process these inputs and outputs into a format suitable for fine-tuning.\n\nThis code processes each sample, creating six translation pairs for each, and writes them to a JSONL file suitable for fine-tuning.\n\nDue to some JSON parsing errors, the final dataset ended up with around 57K rows instead of 60K.\n\nThe next step is to split this dataset into training and testing sets to validate the performance of the model after fine-tuning.\n\nHere's how you can split the dataset:\n\nAfter splitting the dataset, you can convert these JSONL files into HuggingFace datasets to make them easier to work with for fine-tuning:\n\nWith these steps, you now have the train and test datasets ready for fine-tuning and validating your model.\n\n# Finetuning Llama 3 8B using Axolotl\n\nTo give you a brief intro about Axolotl, it is a tool designed to streamline LLM fine-tuning. I like Axolotl because it lets you focus on the data instead of the fine-tuning code, while incorporating the best fine-tuning practices.\n\nIn this project, I used a pretty simple finetuning configuration that I'll provide below. To summarize what the configuration entails:\n\n1. It loads a Llama 3 8B in 8bit\n2. It uses the [alpaca format](https://github.com/tatsu-lab/stanford_alpaca) for finetuning\n3. It finetunes a [LoRA](https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html) adapter instead of doing a full finetune\n4. It uses [sample packing](https://axolotl-ai-cloud.github.io/axolotl/docs/multipack.html) to improve finetuning efficiency\n5. It trains the model for 2 epochs, while running 10 evals per epoch\n6. It logs the train and eval loss into weights and biases\n\nThe finetuning was carried out on a single A5000 GPU (24 GB VRAM) on [Jarvis Labs](https://jarvislabs.ai/) that costs 0.49$/hr, and took around 10 hours to complete. You can check the weights and biases log over [here](https://wandb.ai/ahmedsamirio/en_eg_translator/runs/hwzxxt0r).\n\nFor more info about Axolotl, I highly recommend the [documentation](https://axolotl-ai-cloud.github.io/axolotl/), and checking this [short video guide](https://www.youtube.com/watch?v=HAYPoeC41fw) by [Jarvis Labs](https://jarvislabs.ai/) that shows how to spin up an instance that uses axolotl over there.\n\n```yaml\nbase_model: meta-llama/Meta-Llama-3-8B\nmodel_type: LlamaForCausalLM\ntokenizer_type: AutoTokenizer\n\nload_in_8bit: true\nload_in_4bit: false\nstrict: false\n\ndatasets:\n  - path: translation-dataset-v3-train.hf\n    type: alpaca\n    train_on_split: train\n\ntest_datasets:\n  - path: translation-dataset-v3-test.hf\n    type: alpaca\n    split: train\n\ndataset_prepared_path: ./last_run_prepared\noutput_dir: ./llama_3_translator\nhub_model_id: ahmedsamirio/llama_3_translator_v3\n\n\nsequence_len: 2048\nsample_packing: true\npad_to_sequence_len: true\neval_sample_packing: false\n\nadapter: lora\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_linear: true\nlora_fan_in_fan_out:\nlora_target_modules:\n  - gate_proj\n  - down_proj\n  - up_proj\n  - q_proj\n  - v_proj\n  - k_proj\n  - o_proj\n\nwandb_project: en_eg_translator\nwandb_entity: ahmedsamirio\nwandb_name: llama_3_en_eg_translator_v3\n\ngradient_accumulation_steps: 4\nmicro_batch_size: 2\nnum_epochs: 2\noptimizer: paged_adamw_32bit\nlr_scheduler: cosine\nlearning_rate: 2e-5\n\ntrain_on_inputs: false\ngroup_by_length: false\nbf16: auto\nfp16:\ntf32: false\n\ngradient_checkpointing: true\nearly_stopping_patience:\nresume_from_checkpoint:\nlocal_rank:\nlogging_steps: 1\nxformers_attention:\nflash_attention: true\n\nwarmup_steps: 10\nevals_per_epoch: 10\neval_table_size:\neval_max_new_tokens: 128\nsaves_per_epoch: 1\ndebug:\ndeepspeed:\nweight_decay: 0.0\nfsdp:\nfsdp_config:\nspecial_tokens:\n  pad_token: <|end_of_text|>\n```\n\n\n# Comparing the finetuned model with GPT-4o\n\nOf course, the comparison here will go in facor of GPT-4o, but since we were aiming to emulate it's performance, let's make a comparison to see how far off our finetuned model is.\n\nI'll use three random sample responses from the [Alpaca dataset](https://huggingface.co/datasets/tatsu-lab/alpaca).\n\n### Sample 1\n\n### Sample 2\n\n### Sample 3\n\n# Conclusion\n\nAs you can see, the results are still not perfect. However, the fine-tuned model is starting to catch up with GPT-4o. With some minor adjustments to the fine-tuning methodology, I believe we can achieve performance closer to that of GPT-4o.\n\nIn conclusion, this project demonstrates the potential of fine-tuning large language models to effectively translate English to Egyptian Arabic, addressing a significant gap in existing resources. While the fine-tuned model is not yet on par with GPT-4o, it shows promising results and opens up opportunities for further improvement. By refining the fine-tuning process and expanding the dataset, we can continue to enhance the model's performance. I hope this walkthrough provides valuable insights and inspires others to explore and contribute to this area. Thank you for following along, and I look forward to sharing more updates as this project progresses.\n\n# TL;DR\n\n- Used GPT-4o to create translation pairs from English to Modern Standard Arabic and then to Egyptian Arabic.\n- Generated a dataset from the OpenAssistant/oasst2 messages using GPT-4o and OpenAI's batch API.\n- Prepared and processed the dataset for fine-tuning.\n- Fine-tuned Llama-3-8B using Axolotl, focusing on LoRA adapters and sample packing.\n- Evaluated the fine-tuned model against GPT-4o using sample texts.\n- Found that while the fine-tuned model isn't perfect, it's a significant step towards accessible Egyptian Arabic translations.\n","srcMarkdownNoYaml":"\n\n# Introduction\n\nThis blog post contains a walkthrough of a recent project I worked on to dive deeper into fine-tuning as part of the [`Mastering LLMs: A Conference For Developers & Data Scientists`](https://maven.com/parlance-labs/fine-tuning) course by Hamel Husain and Dan Becker.\n\nWith the abundance of instruction tuning datasets following the release of open source models, many efforts have been made to curate and translate these datasets into Arabic. Unfortunately, Egyptian Arabic dialect is almost forgotten in these attempts, and this project aims to change that.\n\nCurrently, you can use flagship models from OpenAI or Anthropic to translate from English to Egyptian Arabic with excellent results. However, the high cost makes it impractical for individuals to translate the available open source datasets.\n\nThis project's outcome is a translator built using an open source model, making it easier to translate large amounts of data and integrating the Egyptian Arabic dialect into open source models for instruction fine-tuning.\n\n# Outline\n\n#### The blogpost will be split into 3 major parts:\n\n1. Creating finetuning data using GPT-4o\n2. Preparing the dataset for finetuning\n3. Finetuning Llama 3 8B using Axolotl\n4. Comparing the results before and after finetuning\n\n# Creating finetuing data using GPT-4o\n\nTo fine-tune a translation model, we need translation pairs between English and Egyptian Arabic.\n\nSince my goal was to use this model for translating instruction and conversation datasets, I needed a starting point.\n\nI decided to use the OpenAssistant dataset, selecting a random sample of messages rather than entire conversations. In future iterations of this project, I plan to diversify the sources of translation data, using different datasets like Alpaca or even Wikipedia articles.\n\nWhile testing GPT-4o for translating from English to Egyptian Arabic, I noticed that direct translations often resulted in poor quality. To mitigate this, I found that translating first into Modern Standard Arabic and then into Egyptian Arabic produced much better results. Therefore, I used GPT-4o to translate each sentence or paragraph into Arabic first, and then into Egyptian Arabic.\n\nHere's the prompt I used:\n\nAnd here, you can see a sample of GPT-4o's translation using that prompt.\n\nAfter developing the prompt, I utilized the OpenAI batch API to translate a sample of 10K messages from the dataset. The process involved three main steps:\n\n1. Generating 10 separate JSONL files, each containing prompts for translating different messages.\n2. Submitting these JSONL files to OpenAI's batch API.\n3. Downloading the results.\n\nHere’s the code snippet that accomplishes this:\n\n# Preparing the dataset for finetuning\n\nThe output from the previous step consists of separate JSONL files, each containing the output for a specific batch. To proceed with fine-tuning, we need to process this output into a suitable format.\n\nHere's a look at the batch inputs and outputs:\n\nFor each sample in these batch files, we need to extract the English text used for translation and the JSON output from GPT-4o, then parse out the Arabic and Egyptian Arabic translations.\n\nFor each triplet, I create 6 translation pairs:\n\n1. Arabic to English\n2. Egyptian Arabic to English\n3. English to Arabic\n4. Egyptian Arabic to Arabic\n5. English to Egyptian Arabic\n6. Arabic to Egyptian Arabic\n\nThe main purpose of the model was to translate from English to Egyptian Arabic, but I thought that including the back translation could enhance the model's capabilities. Additionally, it creates a bridge for translating from English to Arabic and from Arabic to Egyptian Arabic, which proved effective with GPT-4o.\n\nHere is the updated code to handle this process:\n\nThis snippet loads the batch inputs and outputs, storing them in separate lists. Now we need to process these inputs and outputs into a format suitable for fine-tuning.\n\nThis code processes each sample, creating six translation pairs for each, and writes them to a JSONL file suitable for fine-tuning.\n\nDue to some JSON parsing errors, the final dataset ended up with around 57K rows instead of 60K.\n\nThe next step is to split this dataset into training and testing sets to validate the performance of the model after fine-tuning.\n\nHere's how you can split the dataset:\n\nAfter splitting the dataset, you can convert these JSONL files into HuggingFace datasets to make them easier to work with for fine-tuning:\n\nWith these steps, you now have the train and test datasets ready for fine-tuning and validating your model.\n\n# Finetuning Llama 3 8B using Axolotl\n\nTo give you a brief intro about Axolotl, it is a tool designed to streamline LLM fine-tuning. I like Axolotl because it lets you focus on the data instead of the fine-tuning code, while incorporating the best fine-tuning practices.\n\nIn this project, I used a pretty simple finetuning configuration that I'll provide below. To summarize what the configuration entails:\n\n1. It loads a Llama 3 8B in 8bit\n2. It uses the [alpaca format](https://github.com/tatsu-lab/stanford_alpaca) for finetuning\n3. It finetunes a [LoRA](https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html) adapter instead of doing a full finetune\n4. It uses [sample packing](https://axolotl-ai-cloud.github.io/axolotl/docs/multipack.html) to improve finetuning efficiency\n5. It trains the model for 2 epochs, while running 10 evals per epoch\n6. It logs the train and eval loss into weights and biases\n\nThe finetuning was carried out on a single A5000 GPU (24 GB VRAM) on [Jarvis Labs](https://jarvislabs.ai/) that costs 0.49$/hr, and took around 10 hours to complete. You can check the weights and biases log over [here](https://wandb.ai/ahmedsamirio/en_eg_translator/runs/hwzxxt0r).\n\nFor more info about Axolotl, I highly recommend the [documentation](https://axolotl-ai-cloud.github.io/axolotl/), and checking this [short video guide](https://www.youtube.com/watch?v=HAYPoeC41fw) by [Jarvis Labs](https://jarvislabs.ai/) that shows how to spin up an instance that uses axolotl over there.\n\n```yaml\nbase_model: meta-llama/Meta-Llama-3-8B\nmodel_type: LlamaForCausalLM\ntokenizer_type: AutoTokenizer\n\nload_in_8bit: true\nload_in_4bit: false\nstrict: false\n\ndatasets:\n  - path: translation-dataset-v3-train.hf\n    type: alpaca\n    train_on_split: train\n\ntest_datasets:\n  - path: translation-dataset-v3-test.hf\n    type: alpaca\n    split: train\n\ndataset_prepared_path: ./last_run_prepared\noutput_dir: ./llama_3_translator\nhub_model_id: ahmedsamirio/llama_3_translator_v3\n\n\nsequence_len: 2048\nsample_packing: true\npad_to_sequence_len: true\neval_sample_packing: false\n\nadapter: lora\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_linear: true\nlora_fan_in_fan_out:\nlora_target_modules:\n  - gate_proj\n  - down_proj\n  - up_proj\n  - q_proj\n  - v_proj\n  - k_proj\n  - o_proj\n\nwandb_project: en_eg_translator\nwandb_entity: ahmedsamirio\nwandb_name: llama_3_en_eg_translator_v3\n\ngradient_accumulation_steps: 4\nmicro_batch_size: 2\nnum_epochs: 2\noptimizer: paged_adamw_32bit\nlr_scheduler: cosine\nlearning_rate: 2e-5\n\ntrain_on_inputs: false\ngroup_by_length: false\nbf16: auto\nfp16:\ntf32: false\n\ngradient_checkpointing: true\nearly_stopping_patience:\nresume_from_checkpoint:\nlocal_rank:\nlogging_steps: 1\nxformers_attention:\nflash_attention: true\n\nwarmup_steps: 10\nevals_per_epoch: 10\neval_table_size:\neval_max_new_tokens: 128\nsaves_per_epoch: 1\ndebug:\ndeepspeed:\nweight_decay: 0.0\nfsdp:\nfsdp_config:\nspecial_tokens:\n  pad_token: <|end_of_text|>\n```\n\n\n# Comparing the finetuned model with GPT-4o\n\nOf course, the comparison here will go in facor of GPT-4o, but since we were aiming to emulate it's performance, let's make a comparison to see how far off our finetuned model is.\n\nI'll use three random sample responses from the [Alpaca dataset](https://huggingface.co/datasets/tatsu-lab/alpaca).\n\n### Sample 1\n\n### Sample 2\n\n### Sample 3\n\n# Conclusion\n\nAs you can see, the results are still not perfect. However, the fine-tuned model is starting to catch up with GPT-4o. With some minor adjustments to the fine-tuning methodology, I believe we can achieve performance closer to that of GPT-4o.\n\nIn conclusion, this project demonstrates the potential of fine-tuning large language models to effectively translate English to Egyptian Arabic, addressing a significant gap in existing resources. While the fine-tuned model is not yet on par with GPT-4o, it shows promising results and opens up opportunities for further improvement. By refining the fine-tuning process and expanding the dataset, we can continue to enhance the model's performance. I hope this walkthrough provides valuable insights and inspires others to explore and contribute to this area. Thank you for following along, and I look forward to sharing more updates as this project progresses.\n\n# TL;DR\n\n- Used GPT-4o to create translation pairs from English to Modern Standard Arabic and then to Egyptian Arabic.\n- Generated a dataset from the OpenAssistant/oasst2 messages using GPT-4o and OpenAI's batch API.\n- Prepared and processed the dataset for fine-tuning.\n- Fine-tuned Llama-3-8B using Axolotl, focusing on LoRA adapters and sample packing.\n- Evaluated the fine-tuned model against GPT-4o using sample texts.\n- Found that while the fine-tuned model isn't perfect, it's a significant step towards accessible Egyptian Arabic translations.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"notebook.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.54","theme":"cosmo","title-block-banner":true,"title":"Finetuing an Egyptian Arabic Translator","author":"Ahmed Samir","date":"2024-07-20"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Finetuing an Egyptian Arabic Translator\n",
    "author: Ahmed Samir\n",
    "date: 'July 21nd, 2024'\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following blogpost contains a walkthrough of a recent project that I worked on to learn more about finetuning as part of the [`Mastering LLMs: A Conference For Developers & Data Scientists`](https://maven.com/parlance-labs/fine-tuning) course by Hamel Husain and Dan Becker.\n",
    "\n",
    "Instruction tuning datasets have become abundant since the release of open source models, and many individual and group attempts have been made to curate and translate these datasets into Arabic.\n",
    "\n",
    "Unfortunately, Egyptian Arabic dialect is almost forgotten in these attempts, and this project is an attempt to drive a change in this direction.\n",
    "\n",
    "At the moment, you can use a flagship model from either OpenAI or Anthropic to translate from English to Egyptian Arabic and obtain really nice results, but the large cost makes it unfeasible for individuals to translate the available opensource datasets.\n",
    "\n",
    "The outcome of this project is creating this translator using open source model to make it easier to translate the massive amounts of data available and integrating the Egyptian Arabic dialect into open source models instruction finetuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The blogpost will be split into 3 major parts:\n",
    "\n",
    "1. Creating finetuning data using GPT-4o\n",
    "2. Preparing the dataset for finetuning\n",
    "3. Finetuning Llama 3 8B using Axolotl\n",
    "4. Comparing the results before and after finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating finetuing data using GPT-4o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to finetune a translation model, we need to have translation pairs between English and Egyptian Arabic.\n",
    "\n",
    "Since my intention with this model was to use it in translating instruction and conversation datasets, I had to choose something to begin with. \n",
    "\n",
    "To be honest, I arbitrarly chose the open assistant dataset, and even took a random sample from the messages instead of sampling entire conversations. Anyways, in future iterations on this project, I plan to diversify the sources of translation used for finetuning, for example using different datasets such as alpaca, or using wikipedia articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, it's possible to fix runny mayonnaise! The most common reason for mayonnaise becoming runny is because the oil was added too quickly or the egg yolk wasn't emulsified properly. Here are some steps you can take to fix it:\n",
      "\n",
      "1. Separate another egg yolk and place it in a clean, dry bowl.\n",
      "2. Slowly add the runny mayonnaise to the egg yolk while whisking vigorously.\n",
      "3. Once all the runny mayonnaise has been added, continue whisking until the mixture has emulsified and thickened.\n",
      "4. If the mayonnaise is still too runny, you can add another egg yolk and repeat the process.\n",
      "\n",
      "If the mayonnaise still won't thicken, you can try adding a small amount of dijon mustard or vinegar to the mixture, which can act as emulsifiers and help stabilize the mayonnaise. It's important to add these ingredients slowly and in small amounts to avoid over-thinning the mixture.\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: false\n",
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load open assistant 2 dataset\n",
    "ds = load_dataset(\"OpenAssistant/oasst2\")\n",
    "\n",
    "# Filter for only english\n",
    "english_ds = ds.filter(lambda x: x[\"lang\"] == \"en\")[\"train\"]\n",
    "\n",
    "# Visual check\n",
    "print(english_ds[\"text\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "While testing GPT-4o for translating from English to Egyptian Arabic, I noticed that GPT-4o tends to generate bad translations if asked to direct translation, which could be mitigated by first translating into Arabic, and then translating into Egyptian Arabic. Therefore, I used GPT-4o to translate the sentence of paragraph into Arabic, and then to Egyptian Arabic.\n",
    "\n",
    "You can check the prompt used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: false\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an fluent speaker and expert translator for English, Arabic and Egyptian Arabic. \\\n",
    "Your task is to translate text from English into Egyptian Arabic dialect.\n",
    "\n",
    "# Steps to Achieve the Best Results:\n",
    "Step 1: Translate the text from English into Modern Standard Arabic.\n",
    "Step 2: Translate the text from Modern Standard Arabic into Egyptian dialect.\n",
    "\n",
    "# Adhere to the Following Instructions:\n",
    "1. **Always follow the steps presented above.**\n",
    "2. **Output the two translations as keys in a JSON object:**\n",
    "    - \"ar\" for the Modern Standard Arabic translation.\n",
    "    - \"eg\" for the Egyptian Arabic dialect translation.\n",
    "3. **You may change the order of sentences when necessary** to better mimic the style of Arabic and Egyptian dialect.\n",
    "4. **Your translation should not be literal**; it should capture the essence of the text.\n",
    "5. **Translate specific English terminologies (e.g., science, computer science, biology) or entities \\\n",
    "(e.g., movies, series, poems, names, programming languages)**, but always keep their original English form within parentheses.\n",
    "6. **If the text contains code or is entirely code**, do not translate the code part; write it as it is.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here, you can see a sample of GPT-4o's translation using that prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"ar\": \"نعم، من الممكن إصلاح المايونيز السائل! السبب الأكثر شيوعًا لأن يصبح المايونيز سائلاً هو أن الزيت أضيف بسرعة كبيرة أو أن صفار البيض لم يتم استحلابه بشكل صحيح. إليك بعض الخطوات التي يمكنك اتباعها لإصلاحه:\n",
      "\n",
      "1. افصل صفار بيضة أخرى وضعه في وعاء نظيف وجاف.\n",
      "2. أضف المايونيز السائل تدريجياً إلى صفار البيض مع الخفق بقوة.\n",
      "3. بمجرد إضافة كل المايونيز السائل، استمر في الخفق حتى يتم استحلاب الخليط ويثخن.\n",
      "4. إذا كان المايونيز لا يزال سائلاً جدًا، يمكنك إضافة صفار بيضة أخرى وتكرار العملية.\n",
      "\n",
      "إذا لم يثخن المايونيز بعد، يمكنك محاولة إضافة كمية صغيرة من خردل (dijon) أو خل إلى الخليط، حيث يمكن أن تعمل كمستحلبات وتساعد في تثبيت المايونيز. من المهم إضافة هذه المكونات ببطء وبكميات صغيرة لتجنب تخفيف الخليط بشكل زائد.\",\n",
      "  \n",
      "  \"eg\": \"أيوه، ممكن تصلح المايونيز السايل! أكتر سبب بيخلي المايونيز يبقى سايل هو إن الزيت أضيف بسرعة أو إن صفار البيض مش متجانس كويس. دي شوية خطوات ممكن تعملها لتصلح المشكلة:\n",
      "\n",
      "1. اعزل صفار بيضة تانية وحطه في طبق نضيف وجاف.\n",
      "2. أضف المايونيز السايل تدريجياً لصفار البيض وأنت بتخفق بكل قوة.\n",
      "3. لما تضيف كل المايونيز السايل، كمل في الخفق لحد ما الخليط يتجانس ويكثف.\n",
      "4. لو المايونيز لسه سايل جداً، ممكن تضيف صفار بيضة تانية وتكرر العملية.\n",
      "\n",
      "لو المايونيز لسه مش عايز يثخن، جرب تضيف شوية من خردل (dijon) أو خل للخليط، دول بيساعدوا في تجانس المايونيز. المهم تضيف المكونات دي ببطء وبكميات صغيرة عشان ما تدفيش الخليط.\" \n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: false\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o\",\n",
    "  messages=[{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, \n",
    "            {\"role\": \"user\", \"content\": f\"Translate the following text:\\n{english_ds[\"text\"][1]}\"}],\n",
    "  temperature=1,\n",
    "  max_tokens=512,\n",
    "  top_p=1\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After developing the prompt, I utilized the openai batch api to directly translate a sample of 10K messages from the dataset as follows.\n",
    "\n",
    "The following snippet simply does 3 things:\n",
    "1. It generate 10 separate jsonl files, where each file contains prompts separate requests for translating different messages.\n",
    "2. It submits these jsonl files to openai's batch api\n",
    "3. It downloads their results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading (0, 999) batch outputs\n",
      "Downloading (1000, 1999) batch outputs\n",
      "Downloading (2000, 2999) batch outputs\n",
      "Downloading (3000, 3999) batch outputs\n",
      "Downloading (4000, 4999) batch outputs\n",
      "Downloading (5000, 5999) batch outputs\n",
      "Downloading (6000, 6999) batch outputs\n",
      "Downloading (7000, 7999) batch outputs\n",
      "Downloading (8000, 8999) batch outputs\n",
      "Downloading (9000, 9999) batch outputs\n"
     ]
    }
   ],
   "source": [
    "#| echo: false\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def generate_jsonl(filename, texts):\n",
    "    \"\"\"\n",
    "    Generate a JSONL file with the specified filename\n",
    "    \"\"\"\n",
    "\n",
    "    # Write jsonl file\n",
    "    with open(filename, 'w') as file:\n",
    "        for index in tqdm(range(0, len(texts)), desc=\"Generating JSONL File\"):\n",
    "            text = texts[index]\n",
    "            request = {\n",
    "                \"custom_id\": str(index),\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": \"gpt-4o\",\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                        {\"role\": \"user\", \"content\": f\"Translate the following text:\\n{text}\"}\n",
    "                    ],\n",
    "                    \"temperature\": 1,\n",
    "                    \"top_p\": 1,\n",
    "                    \"max_tokens\": 2048,\n",
    "                }\n",
    "            }\n",
    "            file.write(json.dumps(request) + '\\n')\n",
    "\n",
    "batch_ids = {}\n",
    "\n",
    "for start in range(0, 10000, 1000):\n",
    "    end = start + 1000\n",
    "    english_ds_sample = english_ds.shuffle(seed=42)[\"train\"].select(range(start, end))\n",
    "    english_ds_sample_df = english_ds_sample.to_pandas()\n",
    "\n",
    "    if (start, end-1) in batch_ids.keys():\n",
    "        batch_status = client.batches.retrieve(batch_ids[(start, end-1)]).status\n",
    "        if batch_status != 'failed':\n",
    "            continue\n",
    "\n",
    "    print(f'Creating batch for ({start}, {end-1})')\n",
    "\n",
    "    batch_input_fn = f'batch_api_input_{start}_{end-1}.jsonl'\n",
    "    generate_jsonl(batch_input_fn, english_ds_sample_df[\"text\"].tolist())\n",
    "    \n",
    "    batch_input_file = client.files.create(\n",
    "        file=open(batch_input_fn, \"rb\"),\n",
    "        purpose=\"batch\"\n",
    "    )\n",
    "\n",
    "    batch_input_file_id = batch_input_file.id\n",
    "\n",
    "    batch = client.batches.create(\n",
    "        input_file_id=batch_input_file_id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\n",
    "        \"description\": f\"OpenAssistant 1K ({start}, {end-1}) sample translation\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    batch_ids[(start, end-1)] = batch.id\n",
    "\n",
    "\n",
    "# To run this part, you have to wait for some time to check that all batches are completed\n",
    "for r, batch_id in batch_ids.items():\n",
    "    start, end = r\n",
    "    print(f'Downloading ({start}, {end}) batch outputs')\n",
    "    content = client.files.content(client.batches.retrieve(batch_id).output_file_id)\n",
    "    content.write_to_file(f\"batch_output_{start}_{end}.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the dataset for finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from the previous step consists of separate jsonl files, where each files contains the output for a specific batch. In order to proceed to the finetuning part, we need to process this output into a suitable format that we can deal with.\n",
    "\n",
    "Let's take a look into the batch inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'custom_id': '0',\n",
       "   'method': 'POST',\n",
       "   'url': '/v1/chat/completions',\n",
       "   'body': {'model': 'gpt-4o',\n",
       "    'messages': [{'role': 'system',\n",
       "      'content': 'You are an fluent speaker and expert translator for English, Arabic and Egyptian Arabic. Your task is to translate text from English into Egyptian Arabic dialect.\\n\\n# Steps to Achieve the Best Results:\\nStep 1: Translate the text from English into Modern Standard Arabic.\\nStep 2: Translate the text from Modern Standard Arabic into Egyptian dialect.\\n\\n# Adhere to the Following Instructions:\\n1. **Always follow the steps presented above.**\\n2. **Output the two translations as keys in a JSON object:**\\n    - \"ar\" for the Modern Standard Arabic translation.\\n    - \"eg\" for the Egyptian Arabic dialect translation.\\n3. **You may change the order of sentences when necessary** to better mimic the style of Arabic and Egyptian dialect.\\n4. **Your translation should not be literal**; it should capture the essence of the text.\\n5. **Translate specific English terminologies (e.g., science, computer science, biology) or entities (e.g., movies, series, poems, names, programming languages)**, but always keep their original English form within parentheses.\\n6. **If the text contains code or is entirely code**, do not translate the code part; write it as it is.'},\n",
       "     {'role': 'user',\n",
       "      'content': \"Translate the following text:\\nThanks for the list! I'm especially interested in how these women overcame obstacles in their lives. Are there any resources you can recommend for learning more about their stories?\"}],\n",
       "    'temperature': 1,\n",
       "    'top_p': 1,\n",
       "    'max_tokens': 2048}}],\n",
       " [{'id': 'batch_req_tuugwkbZhxWZ7fKL5YRQWvHP',\n",
       "   'custom_id': '0',\n",
       "   'response': {'status_code': 200,\n",
       "    'request_id': '161ad42423c0ecf010ce879f31e50e5d',\n",
       "    'body': {'id': 'chatcmpl-9b76MvbpkO4dfI8dQLWOPFEPo1dZC',\n",
       "     'object': 'chat.completion',\n",
       "     'created': 1718632462,\n",
       "     'model': 'gpt-4o-2024-05-13',\n",
       "     'choices': [{'index': 0,\n",
       "       'message': {'role': 'assistant',\n",
       "        'content': '```json\\n{\\n  \"msa\": \"شكرًا على القائمة! أنا مهتم بشكل خاص في كيفية تغلب هؤلاء النساء على العقبات في حياتهن. هل هناك أي موارد يمكنك أن توصي بها لمعرفة المزيد عن قصصهن؟\",\\n  \"ea\": \"شكرًا على القائمة! أنا مهتمة بالذات أعرف إزاي الستات دول قدروا يعدوا العقبات اللي في حياتهم. في مصادر تنصحني بيها عشان أتعرف أكتر على حكاويهم؟\"\\n}\\n```'},\n",
       "       'logprobs': None,\n",
       "       'finish_reason': 'stop'}],\n",
       "     'usage': {'prompt_tokens': 290,\n",
       "      'completion_tokens': 109,\n",
       "      'total_tokens': 399},\n",
       "     'system_fingerprint': 'fp_319be4768e'}},\n",
       "   'error': None}])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: false\n",
    "\n",
    "with open('data/batch_api_input_0_999.jsonl') as f:\n",
    "    english = [json.loads(line) for line in f]\n",
    "\n",
    "with open('data/batch_output_0_999.jsonl') as f:\n",
    "    translations = [json.loads(line) for line in f]\n",
    "\n",
    "english[:1], translations[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for each sample in these batch files, we need to extract the English text used for translation, and the JSON output from GPT-4o, and parse out the Arabic and Egyptian Arabic translations.\n",
    "\n",
    "For each triplet, I create 6 translation pairs:\n",
    "1. Arabic to English\n",
    "2. Egyptian Arabic to English\n",
    "3. English to Arabic\n",
    "4. Egyptian Arabic to Arabic\n",
    "5. English to Egyptian Arabic\n",
    "6. Arabic to Egyptian Arabic\n",
    "\n",
    "The main purpose of the model was to translate from English to Egyptian Arabic, but I though that including the back translation could enhance the model capabilities, and also produce the bridge of translating from English to Arabic, and from Arabic to Egyptian Arabic, which proved to do well with GPT-4o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: false\n",
    "\n",
    "import glob\n",
    "\n",
    "def convert_text_to_dict(text):\n",
    "    # Remove the '```json' and '```' delimiters\n",
    "    cleaned_text = text.replace('```json\\n', '').replace('```', '').strip()\n",
    "    \n",
    "    # Convert the cleaned text into a dictionary, ensuring it does not error out\n",
    "    try:\n",
    "        result_dict = json.loads(cleaned_text, strict=False)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"JSON Decode Error:\", e)\n",
    "        # Attempt to clean the text further or handle specific issues\n",
    "        cleaned_text = cleaned_text.replace('\\n', '\\\\n').replace('\\\\\"', '\"').replace('\\\\\\'', \"'\")\n",
    "        try:\n",
    "            result_dict = json.loads(cleaned_text, strict=False)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(\"JSON Decode Error after further cleaning:\", e)\n",
    "            return None\n",
    "    \n",
    "    return result_dict\n",
    "\n",
    "\n",
    "input_files = sorted(glob.glob(\"data/batch_api_input*\"))\n",
    "output_files = sorted(glob.glob(\"data/batch_output*\"))\n",
    "\n",
    "english = []\n",
    "translations = []\n",
    "\n",
    "for input_file in input_files:\n",
    "    with open(input_file) as f:\n",
    "        english.extend([json.loads(line) for line in f])\n",
    "\n",
    "for output_file in output_files:\n",
    "    with open(output_file) as f:\n",
    "        translations.extend([json.loads(line) for line in f])\n",
    "\n",
    "data = []\n",
    "fail = 0\n",
    "i = 0\n",
    "\n",
    "for english_data, arabic_data in tqdm(zip(english, translations)):\n",
    "\n",
    "    try:\n",
    "        output_dict = convert_text_to_dict(arabic_data['response']['body']['choices'][0]['message']['content'])\n",
    "        ar_text = output_dict['ar']\n",
    "        eg_text = output_dict['eg']\n",
    "\n",
    "        en_text = english_data['body']['messages'][-1]['content'].split('Translate the following text:\\n')[-1]\n",
    "        \n",
    "        data.append({\"instruction\": \"Translate the following text to English.\",\n",
    "                    \"input\": ar_text,\n",
    "                    \"output\": en_text,\n",
    "                    \"input_lang\": \"ar\",\n",
    "                    \"output_lang\": \"en\",\n",
    "                    \"id\": i})\n",
    "        \n",
    "        data.append({\"instruction\": \"Translate the following text to English.\",\n",
    "                    \"input\": eg_text,\n",
    "                    \"output\": en_text,\n",
    "                    \"input_lang\": \"eg\",\n",
    "                    \"output_lang\": \"en\",\n",
    "                    \"id\": i})\n",
    "        \n",
    "        data.append({\"instruction\": \"Translate the following text to Arabic.\",\n",
    "                    \"input\": eg_text,\n",
    "                    \"output\": ar_text,\n",
    "                    \"input_lang\": \"eg\",\n",
    "                    \"output_lang\": \"ar\",\n",
    "                    \"id\": i})\n",
    "        \n",
    "        data.append({\"instruction\": \"Translate the following text to Arabic.\",\n",
    "                    \"input\": en_text,\n",
    "                    \"output\": ar_text,\n",
    "                    \"input_lang\": \"en\",\n",
    "                    \"output_lang\": \"ar\",\n",
    "                    \"id\": i})\n",
    "        \n",
    "        data.append({\"instruction\": \"Translate the following text to Egyptian Arabic.\",\n",
    "                    \"input\": ar_text,\n",
    "                    \"output\": eg_text,\n",
    "                    \"input_lang\": \"ar\",\n",
    "                    \"output_lang\": \"eg\",\n",
    "                    \"id\": i})\n",
    "        \n",
    "        data.append({\"instruction\": \"Translate the following text to Egyptian Arabic.\",\n",
    "                    \"input\": en_text,\n",
    "                    \"output\": eg_text,\n",
    "                    \"input_lang\": \"en\",\n",
    "                    \"output_lang\": \"eg\",\n",
    "                    \"id\": i})\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "    except:\n",
    "        fail += 1\n",
    "\n",
    " # Write jsonl file\n",
    "with open(\"data/translation-dataset-openai-10k.jsonl\", 'w') as file:\n",
    "    for index in tqdm(range(0, len(data)), desc=\"Generating JSONL File\"):\n",
    "        row = data[index]\n",
    "        file.write(json.dumps(row) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to some JSON parsing errors, the final datasset was around 57K rows instead of 60K.\n",
    "\n",
    "The next step was to split this dataset into train and test, in order to be able to validate the performance of the model after finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: false\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Path to your .jsonl file\n",
    "dataset_path = 'data/translation-dataset-openai-10k.jsonl'\n",
    "train_dataset_path = 'data/translation-dataset-openai-10k-train.jsonl'\n",
    "test_dataset_path = 'data/translation-dataset-openai-10k-test.jsonl'\n",
    "\n",
    "# Initialize an empty list to store the data\n",
    "train_data_list = []\n",
    "test_data_list = []\n",
    "\n",
    "# Open the file and read line by line\n",
    "with open(dataset_path, 'r', encoding='utf-8') as file:\n",
    "\n",
    "    # Sample test ids\n",
    "    lines = file.readlines()\n",
    "    test_ids = random.sample(list(range(len(lines))), k=len(lines)//10)\n",
    "\n",
    "    for line in lines:\n",
    "        data = json.loads(line.strip())  # Parse JSON from each line\n",
    "        if data[\"id\"] in test_ids:\n",
    "            test_data_list.append(line)\n",
    "        else:\n",
    "            train_data_list.append(line)\n",
    "\n",
    "train_data_list = list([json.loads(l.strip()) for l in set(train_data_list)])\n",
    "test_data_list = list([json.loads(l.strip()) for l in set(test_data_list)])\n",
    "\n",
    "with open(train_dataset_path, 'w') as file:\n",
    "    for index in tqdm(range(0, len(train_data_list)), desc=\"Generating Train JSONL File\"):\n",
    "        row = train_data_list[index]\n",
    "        file.write(json.dumps(row) + '\\n')\n",
    "\n",
    "with open(test_dataset_path, 'w') as file:\n",
    "    for index in tqdm(range(0, len(test_data_list)), desc=\"Generating Train JSONL File\"):\n",
    "        row = test_data_list[index]\n",
    "        file.write(json.dumps(row) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the final part was to convert these train and test sets into HuggingFace datasets, so that we can easily use them in Axolotl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: false\n",
    "\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "train_dataset_df = pd.read_json(train_dataset_path, lines=True).astype(str)\n",
    "test_dataset_df = pd.read_json(test_dataset_path, lines=True).astype(str)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_dataset_df)\n",
    "test_dataset = Dataset.from_pandas(test_dataset_df)\n",
    "\n",
    "train_dataset.save_to_disk('translation-dataset-v3-train.hf')\n",
    "test_dataset.save_to_disk('translation-dataset-v3-test.hf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning Llama 3 8B using Axolotl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give you a brief intro about Axolotl, it is a tool designed to streamline LLM finetuning. I like Axolotl because it lets you focus on the data instead of focusing on the finetuning code, while having the best finetuning practicies built-in.\n",
    "\n",
    "In this project, I used a pretty simple finetuning configuration that I'll provide below. To summarize what the configuration entails:\n",
    "1. It loads a Llama 3 8B in 8bit\n",
    "2. It uses the [alpaca format](https://github.com/tatsu-lab/stanford_alpaca) for finetuning\n",
    "3. It finetunes a [LoRA](https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html) adapter instead of doing a full finetune\n",
    "4. It uses [sample packing](https://axolotl-ai-cloud.github.io/axolotl/docs/multipack.html) to improve finetuning efficiency\n",
    "5. It trains the model for 2 epochs, while running 10 evals per epoch\n",
    "6. It logs the train and eval loss into weights and biases\n",
    "\n",
    "The finetuning was carried out on a single A5000 GPU (24 GB VRAM) on [Jarvis Labs](https://jarvislabs.ai/) that costs 0.49$/hr, and took around 10 hours to complete. You can check the weights and biases log over [here](https://wandb.ai/ahmedsamirio/en_eg_translator/runs/hwzxxt0r).\n",
    "\n",
    "For more info about Axolotl, I highly recommend the [documentation](https://axolotl-ai-cloud.github.io/axolotl/), and checking this [short video guide](https://www.youtube.com/watch?v=HAYPoeC41fw) by [Jarvis Labs](https://jarvislabs.ai/) that shows how to spin up an instance that uses axolotl over there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "base_model: meta-llama/Meta-Llama-3-8B\n",
    "model_type: LlamaForCausalLM\n",
    "tokenizer_type: AutoTokenizer\n",
    "\n",
    "load_in_8bit: true\n",
    "load_in_4bit: false\n",
    "strict: false\n",
    "\n",
    "datasets:\n",
    "  - path: translation-dataset-v3-train.hf\n",
    "    type: alpaca\n",
    "    train_on_split: train\n",
    "\n",
    "test_datasets:\n",
    "  - path: translation-dataset-v3-test.hf\n",
    "    type: alpaca\n",
    "    split: train\n",
    "\n",
    "dataset_prepared_path: ./last_run_prepared\n",
    "output_dir: ./llama_3_translator\n",
    "hub_model_id: ahmedsamirio/llama_3_translator_v3\n",
    "\n",
    "\n",
    "sequence_len: 2048\n",
    "sample_packing: true\n",
    "pad_to_sequence_len: true\n",
    "eval_sample_packing: false\n",
    "\n",
    "adapter: lora\n",
    "lora_r: 32\n",
    "lora_alpha: 16\n",
    "lora_dropout: 0.05\n",
    "lora_target_linear: true\n",
    "lora_fan_in_fan_out:\n",
    "lora_target_modules:\n",
    "  - gate_proj\n",
    "  - down_proj\n",
    "  - up_proj\n",
    "  - q_proj\n",
    "  - v_proj\n",
    "  - k_proj\n",
    "  - o_proj\n",
    "\n",
    "wandb_project: en_eg_translator\n",
    "wandb_entity: ahmedsamirio\n",
    "wandb_name: llama_3_en_eg_translator_v3\n",
    "\n",
    "gradient_accumulation_steps: 4\n",
    "micro_batch_size: 2\n",
    "num_epochs: 2\n",
    "optimizer: paged_adamw_32bit\n",
    "lr_scheduler: cosine\n",
    "learning_rate: 2e-5\n",
    "\n",
    "train_on_inputs: false\n",
    "group_by_length: false\n",
    "bf16: auto\n",
    "fp16:\n",
    "tf32: false\n",
    "\n",
    "gradient_checkpointing: true\n",
    "early_stopping_patience:\n",
    "resume_from_checkpoint:\n",
    "local_rank:\n",
    "logging_steps: 1\n",
    "xformers_attention:\n",
    "flash_attention: true\n",
    "\n",
    "warmup_steps: 10\n",
    "evals_per_epoch: 10\n",
    "eval_table_size:\n",
    "eval_max_new_tokens: 128\n",
    "saves_per_epoch: 1\n",
    "debug:\n",
    "deepspeed:\n",
    "weight_decay: 0.0\n",
    "fsdp:\n",
    "fsdp_config:\n",
    "special_tokens:\n",
    "  pad_token: <|end_of_text|>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the finetuned model with GPT-4o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the comparison here will go in facor of GPT-4o, but since we were aiming to emulate it's performance, let's make a comparison to see how far off our finetuned model is.\n",
    "\n",
    "I'll use three random sample responses from the [alpaca dataset](https://huggingface.co/datasets/tatsu-lab/alpaca)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_sample = [\n",
    "    \"\"\"I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client’s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team’s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client’s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities.\"\"\",\n",
    "    \"\"\"There are several factors that contribute to an individual's success, such as hard work and dedication, effective communication skills, positive attitude, good time management, a clear vision and specific goals, problem-solving and decision-making skills, willingness to take risks, resilience and adaptability, prioritization and organization, proactivity, self-motivation, personal growth, and the ability to collaborate with others.\"\"\",\n",
    "    \"\"\"Cats and dogs are both beloved pets, but they have important differences. Dogs are typically more outgoing and energetic, while cats are considered more independent. Dogs tend to be more social and active, enjoying walks and playing with other animals. Cats, on the other hand, tend to be more solitary, preferring to relax and snuggle up in a warm spot. Dogs typically require more care and attention, while cats are more self-sufficient. Despite these differences, cats and dogs remain popular and loving pets.\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: false\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ahmedsamirio/Egyptian-Arabic-Translator-Llama-3-8B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"ahmedsamirio/Egyptian-Arabic-Translator-Llama-3-8B\", load_in_8bit=True, device_map=\"cuda\")\n",
    "pipe = pipeline(task='text-generation', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: false\n",
    "\n",
    "ar_template = \"\"\"<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Translate the following text to Arabic.\n",
    "\n",
    "### Input:\n",
    "{text}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "eg_template = \"\"\"<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Translate the following text to Egyptian Arabic.\n",
    "\n",
    "### Input:\n",
    "{text}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "def get_output(prompt, max_new_tokens):\n",
    "    out = pipe(prompt, max_new_tokens=max_new_tokens, do_sample=False, temperature=None)\n",
    "    return out[0]['generated_text'].split(\"### Response:\\n\")[-1]\n",
    "\n",
    "def get_ft_model_translations(text):\n",
    "    eg_text = get_output(eg_template.format(text=text), 512)\n",
    "    translations = {\"eg\": eg_text}\n",
    "    return translations\n",
    "\n",
    "def get_gpt4o_translations(text):\n",
    "    response = client.chat.completions.create(\n",
    "      model=\"gpt-4o\",\n",
    "      messages=[{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, \n",
    "                {\"role\": \"user\", \"content\": f\"Translate the following text:\\n{text}\"}],\n",
    "      temperature=1,\n",
    "      max_tokens=512,\n",
    "      top_p=1\n",
    "    )\n",
    "    translations = convert_text_to_dict(response.choices[0].message.content)\n",
    "    return translations\n",
    "\n",
    "def compare_translations(text):\n",
    "    ft_translations = get_ft_model_translations(text)\n",
    "    gpt4o_translations = get_gpt4o_translations(text)\n",
    "    \n",
    "    print(\"Original Text:\\n\")\n",
    "    print(text)\n",
    "    print()\n",
    "    \n",
    "    print('GPT-4o Translation:\\n')\n",
    "    print(gpt4o_translations['eg'])\n",
    "    print()\n",
    "    \n",
    "    print('Fintuned Model Translation:\\n')\n",
    "    print(ft_translations['eg'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "\n",
      "I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client’s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team’s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client’s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities.\n",
      "\n",
      "GPT-4o Translation:\n",
      "\n",
      "اضطريت آخد قرار صعب وأنا كنت شغال كمدير مشروع في شركة مقاولات. كنت مسؤول عن مشروع لازم يخلص في معاد معين عشان نرضي العميل. بس بسبب تأخيرات غير متوقعة، ماقدرناش نلتزم بالميعاد، فكان لازم آخد قرار صعب. قررت أمد الميعاد، بس كان لازم أضغط على فريق العمل وأزود الميزانية. رغم إن القرار كان محفوف بالمخاطر، قررت أمشي فيه عشان المشروع يخلص في وقته ونرضي العميل. في الآخر، المشروع نجح وتنفذ بنجاح وده كان شهادة على قدراتي في القيادة واتخاذ القرارات.\n",
      "\n",
      "Fintuned Model Translation:\n",
      "\n",
      "كان لازم أاخد قرار صعب لما كنت شغال كمدير مشروع في شركة بناء. كنت مسؤول عن مشروع كان محتاج يخلص في موعد معين عشان نوفر توقعات العميل. بس بسبب تأخيرات غير متوقعة، ما قدرتش نتوفر على الميعاد واضطرت أاخد قرار صعب. قررت أطول الميعاد، بس كان لازم أزود موارد الفريق أكتر وأزود الميزانية. رغم إن القرار كان محفوف بالمخاطر، قررت أتابع المهمة عشان أتأكد إن المشروع يخلص في الوقت المناسب وإن توقعات العميل تتحقق. المشروع خلص في النهاية بنجاح وده اتشاف كدليل على قيادتي وقدراتي في اتخاذ القرارات.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_translations(alpaca_sample[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "\n",
      "There are several factors that contribute to an individual's success, such as hard work and dedication, effective communication skills, positive attitude, good time management, a clear vision and specific goals, problem-solving and decision-making skills, willingness to take risks, resilience and adaptability, prioritization and organization, proactivity, self-motivation, personal growth, and the ability to collaborate with others.\n",
      "\n",
      "GPT-4o Translation:\n",
      "\n",
      "في عوامل كتير بتساهم في نجاح الشخص، زي الشغل الجامد والاجتهاد، مهارات التواصل الفعّالة، النظرة الإيجابية، إدارة الوقت بشكل كويس، رؤية واضحة وأهداف محددة، مهارات حل المشاكل واتخاذ القرار، الرغبة في المخاطرة، المرونة والتكيف، الأولويات والتنظيم، المبادرة، التحفيز الذاتي، النمو الشخصي، والقدرة على التعاون مع الناس التانية.\n",
      "\n",
      "Fintuned Model Translation:\n",
      "\n",
      "فيه عوامل كتير بتساهم في نجاح الشخص، زي الشغل الجاد والتفاني، مهارات التواصل الفعّالة، المزاج الإيجابي، إدارة الوقت بشكل كويس، رؤية واضحة وأهداف محددة، مهارات حل المشاكل واتخاذ القرارات، استعداد لتحمل المخاطر، الصمود والقدرة على التكيف، الترتيب والتنظيم، الإقدام، التحفيز الذاتي، النمو الشخصي، والقدرة على التعاون مع الآخرين.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_translations(alpaca_sample[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "\n",
      "Cats and dogs are both beloved pets, but they have important differences. Dogs are typically more outgoing and energetic, while cats are considered more independent. Dogs tend to be more social and active, enjoying walks and playing with other animals. Cats, on the other hand, tend to be more solitary, preferring to relax and snuggle up in a warm spot. Dogs typically require more care and attention, while cats are more self-sufficient. Despite these differences, cats and dogs remain popular and loving pets.\n",
      "\n",
      "GPT-4o Translation:\n",
      "\n",
      "القطط والكلاب الحيوانات دي الاتنين محبوبين، بس في اختلافات مهمة بينهم. الكلاب عادةً بتكون أكثر انفتاح ونشاط، في حين إن القطط بتحب تستقل. الكلاب بتحب الاختلاط وبتكون نشيطة، بتمبسط من المشي واللعب مع الحيوانات التانية. لكن القطط بتميل للعزلة، وبتحب تسترخى وتتمدد في مكان دافئ. الكلاب بتطلب رعاية واهتمام أكتر، بس القطط بتعتمد على نفسها أكتر. رغم الاختلافات دي، القطط والكلاب لسه حيوانات أليفة محبوبة وشعبية.\n",
      "\n",
      "Fintuned Model Translation:\n",
      "\n",
      "القطط والكلاب هما حيوانات أليفة محبوبة، بس عندهم اختلافات مهمة. الكلاب عادةً بتكون أكتر نشاطًا وطاقة، والقطط بتعتبر أكتر استقلالية. الكلاب عادةً بتكون أكتر اجتماعية ونشطة، وبتستمتع بالمشي واللعب مع الحيوانات التانية. أما القطط، بتكون أكتر وحدة، وبتفضل تستريح وتدوس في مكان دافي. الكلاب عادةً محتاجة عناية أكتر، والقطط بتكون أكتر استقلالية. رغم الاختلافات دي، القطط والكلاب لسه حيوانات أليفة مشهورة ومحبوبة.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_translations(alpaca_sample[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the results are still not that great, however the finetuned model is almost catching up with GPT-4o, and I believe that with some minor tweaks in the finetuning methodology, we can achieve a performance close to that of GPT-4o."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TL;DR\n",
    "\n",
    "- Used GPT-4o to create translation pairs from English to Modern Standard Arabic and then to Egyptian Arabic.\n",
    "- Generated a dataset from the OpenAssistant/oasst2 messages using GPT-4o and OpenAI's batch API.\n",
    "- Prepared and processed the dataset for fine-tuning.\n",
    "- Fine-tuned Llama-3-8B using Axolotl, focusing on LoRA adapters and sample packing.\n",
    "- Evaluated the fine-tuned model against GPT-4o using sample texts.\n",
    "- Found that while the fine-tuned model isn't perfect, it's a significant step towards accessible Egyptian Arabic translations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
